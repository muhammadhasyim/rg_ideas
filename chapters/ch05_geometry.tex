%===============================================================================
\chapter{Metric Structure on the Extended Theory Space}
\label{ch:geometry}
%===============================================================================

\marginnote{Chapters 1--4 developed the RG as a flow on parameter space. This chapter asks: does parameter space have more structure? The answer is yes, and the structure has profound physical meaning. The metric measures distinguishability between theories, and it extends naturally to the transseries-enlarged space.}

Parameter space is not just a set of points. It has \textbf{geometry}. There is a natural notion of distance between theories, measured by a \textbf{metric}. This metric has deep physical meaning in statistical mechanics and information theory. The RG flow can often be written as \textbf{gradient flow} with respect to this metric, meaning theories flow ``downhill'' toward fixed points.

Even more remarkably, this geometric structure extends to the full theory space including transseries parameters. The metric acquires new components measuring distinguishability along non-perturbative directions. Near Stokes lines, these components become large, reflecting that theories differing in their transseries structure become highly distinguishable.

This chapter develops these ideas, connecting the RG to information geometry and establishing theorems about the irreversibility of the RG flow.

%-------------------------------------------------------------------------------
\section{Why Metrics Matter}
\label{sec:why_metrics}
%-------------------------------------------------------------------------------

A metric on parameter space measures how ``different'' two nearby theories are.

\subsection{The Question}

Consider two theories with coupling constants $g$ and $g + dg$. How different are their physical predictions?

\marginnote{The metric answers: how much do physical predictions change when we change parameters? Large metric means small changes in parameters cause large changes in physics.}

If changing $g$ by $dg$ makes a huge difference, the metric $G_{ab}(g)$ should be large. If changing $g$ barely affects anything, the metric should be small.

\subsection{Formalizing ``Distinguishability''}

In statistical systems, we can make this precise. The output of a theory is a probability distribution $p(x; g)$ over configurations $x$. Two theories are ``distinguishable'' if their distributions are different.

The natural measure of distinguishability between probability distributions is the \textbf{Fisher information metric}, which quantifies how much the log-likelihood changes when parameters change.

\subsection{Physical Significance}

The metric has several physical interpretations.

For statistical inference, the metric determines how well we can estimate parameters from data. Large metric means parameters are easy to estimate because small changes produce noticeable effects.

For thermodynamics, the metric is related to fluctuations. Large metric means large fluctuations in the observables conjugate to the parameters.

For RG, the metric determines the ``natural'' speed of flow. And it enables gradient flow formulations where the RG becomes a descent toward minimum ``potential.''

%-------------------------------------------------------------------------------
\section{The Fisher Information Metric}
\label{sec:fisher}
%-------------------------------------------------------------------------------

The Fisher information metric is the natural metric on a space of probability distributions.

\subsection{Definition}

Given a family of probability distributions $p(x; g)$ parameterized by $g = (g^1, \ldots, g^n)$, the \textbf{Fisher information metric} is:
\begin{equation}
G_{ab}(g) = \int dx \, p(x; g) \, \frac{\partial \log p}{\partial g^a} \, \frac{\partial \log p}{\partial g^b}
\label{eq:fisher_def}
\end{equation}

\marginnote{The Fisher metric measures the expected squared change in log-likelihood. It's positive semi-definite and symmetric.}

This is the covariance matrix of the \textbf{score function} $s_a = \partial\log p/\partial g^a$.

\subsection{Properties}

The Fisher metric has several important properties.

It is \textbf{positive semi-definite} because $G_{ab}v^av^b = \langle(v^as_a)^2\rangle \geq 0$.

It is \textbf{reparameterization covariant} because under $g^a \to g'^a(g)$, we have $G'_{a'b'} = (\partial g^a/\partial g'^{a'})(\partial g^b/\partial g'^{b'})G_{ab}$, which is the transformation law for a tensor.

It is connected to the \textbf{CramÃ©r-Rao bound}, and the inverse $G^{-1}$ bounds the variance of any unbiased estimator for the parameters.

\begin{workedbox}[Box 5.1: Fisher Metric for a Gaussian]
\textbf{Setup:} Consider a Gaussian distribution with mean $\mu$ and variance $\sigma^2$:
\begin{equation}
p(x; \mu, \sigma) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)
\end{equation}

The parameters are $g = (\mu, \sigma)$.

\textbf{Step 1: Compute the log-likelihood.}
\begin{equation}
\log p = -\frac{1}{2}\log(2\pi) - \log\sigma - \frac{(x-\mu)^2}{2\sigma^2}
\end{equation}

\textbf{Step 2: Compute the score functions.}
\begin{align}
\frac{\partial\log p}{\partial\mu} &= \frac{x-\mu}{\sigma^2} \\
\frac{\partial\log p}{\partial\sigma} &= -\frac{1}{\sigma} + \frac{(x-\mu)^2}{\sigma^3}
\end{align}

\textbf{Step 3: Compute the expectations.}
\begin{align}
G_{\mu\mu} &= \left\langle\frac{(x-\mu)^2}{\sigma^4}\right\rangle = \frac{1}{\sigma^2} \\
G_{\mu\sigma} &= \left\langle\frac{(x-\mu)}{\sigma^2}\left(-\frac{1}{\sigma} + \frac{(x-\mu)^2}{\sigma^3}\right)\right\rangle = 0 \\
G_{\sigma\sigma} &= \left\langle\left(-\frac{1}{\sigma} + \frac{(x-\mu)^2}{\sigma^3}\right)^2\right\rangle = \frac{2}{\sigma^2}
\end{align}

\textbf{Result:} The Fisher metric is:
\begin{equation}
G = \frac{1}{\sigma^2}\begin{pmatrix} 1 & 0 \\ 0 & 2 \end{pmatrix}
\end{equation}

\textbf{Physical interpretation:} At small $\sigma$, the metric is large because the distribution is sharply peaked. Small changes in $\mu$ or $\sigma$ produce noticeable differences. At large $\sigma$, the metric is small because the distribution is spread out and changes are harder to detect.
\end{workedbox}

%-------------------------------------------------------------------------------
\section{The Zamolodchikov Metric}
\label{sec:zamolodchikov}
%-------------------------------------------------------------------------------

In quantum field theory, there is a natural metric on the space of theories that is closely related to the Fisher metric.

\subsection{Definition}

Consider a QFT with coupling constants $g^a$ and corresponding operators $\mathcal{O}_a$ that appear in the action as $S = S_0 + \sum_a g^a\int d^dx\,\mathcal{O}_a(x)$.

\marginnote{The Zamolodchikov metric is the two-point function of the operators that ``generate'' deformations of the action.}

The \textbf{Zamolodchikov metric} is:
\begin{equation}
G_{ab}(g) = \lim_{|x-y|\to 0}|x-y|^{2\Delta_a}\langle\mathcal{O}_a(x)\mathcal{O}_b(y)\rangle_g
\label{eq:zamolodchikov}
\end{equation}
where $\Delta_a$ is the scaling dimension of $\mathcal{O}_a$ and the limit extracts the coefficient of the leading singularity.

\subsection{Relation to the Fisher Metric}

In statistical mechanics, the partition function $Z[g] = \int\mathcal{D}\phi\,e^{-S[\phi;g]}$ defines a probability distribution. The Zamolodchikov metric is precisely the Fisher metric for this distribution.

The operators $\mathcal{O}_a$ generate changes in the action, and their two-point function measures how much the distribution changes under these deformations.

\subsection{Properties at Fixed Points}

At a conformal fixed point, the Zamolodchikov metric has special structure.

Operators organize into \textbf{conformal families}. Primary operators and their descendants have specific transformation properties. The metric is diagonal in the basis of primary operators (at the fixed point).

The metric coefficients are related to OPE coefficients, which are physical observables that can be measured experimentally or computed in conformal field theory.

%-------------------------------------------------------------------------------
\section{Metric on the Extended Theory Space}
\label{sec:extended_metric}
%-------------------------------------------------------------------------------

The perturbative theory space $\mathcal{M}_{\text{pert}}$ is a submanifold of the full extended theory space $\mathcal{M}_{\text{ext}}$ that includes transseries parameters. The metric extends naturally to this larger space.

\subsection{The Extended Coordinates}

\marginnote{The full theory space has coordinates $(g^a, \sigma^n)$ combining perturbative couplings and transseries parameters.}

The extended theory space has coordinates $(g^1, \ldots, g^n, \sigma^1, \sigma^2, \ldots)$ where $g^a$ are the perturbative couplings and $\sigma^k$ are the transseries parameters weighting the $k$-instanton sectors.

The metric on the extended space has three types of components.

The first type is $G_{ab}$ with purely perturbative indices. These are the standard Zamolodchikov metric components measuring distinguishability along perturbative directions.

The second type is $G_{\sigma^k\sigma^l}$ with purely transseries indices. These measure distinguishability between theories differing in their non-perturbative content.

The third type is $G_{a\sigma^k}$ with mixed indices. These measure cross-correlations between perturbative and non-perturbative directions.

\subsection{Behavior Near Stokes Lines}

The extended metric has interesting behavior near Stokes lines in the Borel plane.

\marginnote{Near Stokes lines, theories differing in $\sigma$ become highly distinguishable. The metric component $G_{\sigma\sigma}$ diverges.}

Far from Stokes lines, the exponentially suppressed sectors $e^{-nS/g}\sigma^n$ contribute negligibly to physical observables. Theories differing only in $\sigma$ are almost indistinguishable, and $G_{\sigma\sigma}$ is small.

Near a Stokes line, the exponentially small terms become comparable to the error in the perturbative approximation. Different values of $\sigma$ lead to noticeably different predictions, and $G_{\sigma\sigma}$ becomes large.

At a Stokes crossing, the ambiguity in the resummation becomes maximal. The metric component can diverge, signaling that the transseries parameter becomes a crucial degree of freedom.

\begin{workedbox}[Box 5.2: The Extended Metric Near a Stokes Line]
\textbf{Setup:} Consider a toy model where the partition function is:
\begin{equation}
Z(g, \sigma) = Z_{\text{pert}}(g) + \sigma e^{-S/g}Z^{(1)}(g) + \cdots
\end{equation}

\textbf{The log-partition function:}
\begin{equation}
W = \log Z \approx \log Z_{\text{pert}} + \sigma e^{-S/g}\frac{Z^{(1)}}{Z_{\text{pert}}} + O(\sigma^2)
\end{equation}

\textbf{The metric component $G_{\sigma\sigma}$:}
\begin{equation}
G_{\sigma\sigma} = \frac{\partial^2 W}{\partial\sigma^2} - \left(\frac{\partial W}{\partial\sigma}\right)^2 \sim e^{-2S/g}\left(\frac{Z^{(1)}}{Z_{\text{pert}}}\right)^2 - \cdots
\end{equation}

\textbf{Far from Stokes lines:}
When $g$ is real and positive, $e^{-S/g}$ is exponentially small. The metric $G_{\sigma\sigma}$ is negligible, and changing $\sigma$ barely affects predictions.

\textbf{Near a Stokes line:}
When $\text{Im}(g)$ approaches zero from a region where $\text{Re}(e^{-S/g})$ changes sign, the exponential terms become order one compared to the error. The metric $G_{\sigma\sigma}$ grows and indicates that $\sigma$ matters.

\textbf{Physical interpretation:}
The Stokes phenomenon is the transition from $\sigma$ being ``invisible'' (small metric) to $\sigma$ being ``observable'' (large metric).
\end{workedbox}

%-------------------------------------------------------------------------------
\section{Gradient Flow}
\label{sec:gradient_flow}
%-------------------------------------------------------------------------------

A remarkable fact about many RG flows is that they can be written as \textbf{gradient flows}. This means the beta function is the gradient of some ``potential'' with respect to the metric.

\subsection{Definition}

A flow is \textbf{gradient} with respect to a metric $G_{ab}$ and potential $V$ if:
\begin{equation}
\beta^a = -G^{ab}\frac{\partial V}{\partial g^b}
\label{eq:gradient_flow_def}
\end{equation}
where $G^{ab}$ is the inverse metric.

\marginnote{Gradient flow: the RG ``rolls downhill'' on the potential landscape. Fixed points are stationary points of $V$.}

In components with index raised:
\begin{equation}
\frac{dg^a}{d\ell} = -G^{ab}\partial_b V
\end{equation}

\subsection{Consequences of Gradient Structure}

If the RG is gradient flow with $V$ bounded below, then several important consequences follow.

The flow is \textbf{irreversible} in the sense that the potential decreases along trajectories:
\begin{equation}
\frac{dV}{d\ell} = \partial_a V \cdot \beta^a = -G_{ab}\beta^a\beta^b \leq 0
\end{equation}
because $G$ is positive semi-definite.

Fixed points are \textbf{local minima} of $V$ (or saddle points, but not maxima).

No \textbf{limit cycles} can exist because $V$ keeps decreasing.

\subsection{When Is the RG Gradient Flow?}

Not all RG flows are gradient. The condition is:
\begin{equation}
\partial_a\beta_b = \partial_b\beta_a
\end{equation}
where $\beta_a = G_{ab}\beta^b$.

\marginnote{A flow is gradient iff $\beta_a$ is a closed 1-form. This can fail in theories with multiple couplings.}

This says $\beta_a$ must be a \textbf{closed 1-form}. Locally, it's then exact, meaning $\beta_a = -\partial_a V$ for some $V$.

In 2D CFT, Zamolodchikov proved this holds with $V = c$, the central charge. In higher dimensions, the situation is more subtle.

%-------------------------------------------------------------------------------
\section{Zamolodchikov's c-Theorem}
\label{sec:c_theorem}
%-------------------------------------------------------------------------------

The most famous example of gradient flow structure is Zamolodchikov's c-theorem in two-dimensional quantum field theory.

\subsection{The Central Charge}

In 2D CFT, the \textbf{central charge} $c$ is a fundamental quantity that counts degrees of freedom in a specific sense. It appears in the two-point function of the stress tensor:
\begin{equation}
\langle T(z)T(w)\rangle = \frac{c/2}{(z-w)^4}
\end{equation}

\marginnote{The central charge $c$ counts ``degrees of freedom'' in 2D. Free scalars have $c = 1$, free fermions have $c = 1/2$.}

For free theories, $c$ is the number of bosons plus half the number of fermions. For interacting theories, $c$ takes non-integer values.

\subsection{The Theorem}

\begin{theorem}[Zamolodchikov, 1986]
\label{thm:c_theorem}
In any unitary 2D QFT, there exists a function $c(\ell)$ along RG trajectories such that:
\begin{enumerate}
\item $c$ decreases monotonically: $dc/d\ell \leq 0$
\item At fixed points, $c$ equals the central charge of the CFT
\item $dc/d\ell = 0$ only at fixed points
\end{enumerate}
\end{theorem}

\marginnote{The c-theorem: the central charge decreases under RG. This is the ``arrow of scale'' in 2D.}

The proof constructs the c-function explicitly from correlation functions of the stress tensor.

\subsection{Physical Interpretation}

The c-theorem says that ``degrees of freedom'' decrease under coarse-graining. As we zoom out, we lose information about UV details, and the number of effective degrees of freedom decreases.

This makes intuitive sense. Integrating out short-wavelength modes should reduce the complexity of the effective description.

\begin{workedbox}[Box 5.3: The c-Function in Practice]
\textbf{Example: Free scalar to massive scalar.}

A free massless scalar in 2D has $c_{\text{UV}} = 1$.

Add a mass term $m^2\phi^2$. At energies $E \gg m$, the theory looks massless. At energies $E \ll m$, the massive scalar decouples.

\textbf{The flow:}
\begin{equation}
c_{\text{UV}} = 1 \quad \xrightarrow{\text{RG}} \quad c_{\text{IR}} = 0
\end{equation}

The c-function interpolates smoothly between 1 and 0 as we flow from UV to IR.

\textbf{What it measures:} In the UV, one degree of freedom (the scalar). In the IR, zero degrees of freedom (the scalar has decoupled).

\textbf{The gradient flow:}
The beta function satisfies:
\begin{equation}
\beta^a = -G^{ab}\frac{\partial c}{\partial g^b}
\end{equation}
with $c$ as the potential. Fixed points are extrema of $c$.
\end{workedbox}

%-------------------------------------------------------------------------------
\section{Higher-Dimensional Generalizations}
\label{sec:higher_d}
%-------------------------------------------------------------------------------

The c-theorem is specific to 2D. In higher dimensions, similar but weaker results hold.

\subsection{The F-Theorem in 3D}

In 3D CFT, the analog of $c$ is the \textbf{F-quantity}, defined as the free energy on a 3-sphere:
\begin{equation}
F = -\log Z_{S^3}
\end{equation}

\marginnote{The F-theorem: in 3D, the $S^3$ free energy decreases under RG. Proved by Jafferis, Klebanov, Pufu, and Safdi (2011).}

Jafferis and collaborators proved that $F$ decreases under RG:
\begin{equation}
F_{\text{UV}} \geq F_{\text{IR}}
\end{equation}
with equality only if the theories are the same.

\subsection{The a-Theorem in 4D}

In 4D, there are two central charges, $a$ and $c$, appearing in the conformal anomaly:
\begin{equation}
\langle T^\mu_\mu\rangle = \frac{c}{16\pi^2}(\text{Weyl})^2 - \frac{a}{16\pi^2}(\text{Euler})
\end{equation}

Cardy conjectured that $a$ decreases under RG. This was proved by Komargodski and Schwimmer in 2011:
\begin{equation}
a_{\text{UV}} \geq a_{\text{IR}}
\end{equation}

This is the \textbf{a-theorem}, which is the 4D generalization of the c-theorem.

\subsection{The General Pattern}

In all known cases, there exists a quantity that decreases under RG:
\begin{center}
\begin{tabular}{ccc}
\toprule
Dimension & Quantity & Name \\
\midrule
$d = 2$ & $c$ (central charge) & c-theorem \\
$d = 3$ & $F$ ($S^3$ free energy) & F-theorem \\
$d = 4$ & $a$ (Euler anomaly coeff.) & a-theorem \\
\bottomrule
\end{tabular}
\end{center}

This supports the picture of RG as ``information loss'' and suggests a universal structure that transcends dimension.

%-------------------------------------------------------------------------------
\section{The Resurgent c-Function}
\label{sec:resurgent_c}
%-------------------------------------------------------------------------------

The monotonicity theorems apply to the full theory, not just the perturbative sector. This leads to constraints on the transseries structure.

\subsection{Extension to the Full Theory Space}

The c-function on the extended theory space $\mathcal{M}_{\text{ext}}$ includes dependence on transseries parameters:
\begin{equation}
c_{\text{full}}(g, \sigma) = c_{\text{pert}}(g) + \sum_{n=1}^\infty \sigma^n e^{-nS/g}c^{(n)}(g)
\end{equation}

\marginnote{The full c-function is a transseries. Monotonicity applies to the complete object, including non-perturbative sectors.}

The monotonicity theorem says $c_{\text{full}}$ decreases along any RG trajectory, including trajectories that cross Stokes lines.

\subsection{Constraints on Stokes Constants}

The requirement that $c_{\text{full}}$ decreases continuously across Stokes lines places constraints on the Stokes constants.

At a Stokes crossing, $\sigma$ jumps by the Stokes constant $S_1$. The continuity of $c_{\text{full}}$ requires:
\begin{equation}
c_{\text{full}}(g, \sigma + S_1) = c_{\text{full}}(g, \sigma) + O(\text{exponentially small})
\end{equation}

This is automatic if the c-function is correctly defined including all sectors, but it provides a non-trivial consistency check on the resurgent structure.

\subsection{Irreversibility and Non-Perturbative Information}

The c-theorem applies to both perturbative and non-perturbative information. Under coarse-graining, information is lost from both high-energy Feynman diagrams and high-action instanton configurations.

\marginnote{The RG loses both perturbative (short-distance) and non-perturbative (high-action) information as we coarse-grain.}

This has a beautiful interpretation. In the UV, the theory has access to both high-energy perturbative physics and high-action non-perturbative configurations. As we flow to the IR, both types of information are integrated out.

%-------------------------------------------------------------------------------
\section{The Porous Medium Equation as Gradient Flow}
\label{sec:pme_gradient}
%-------------------------------------------------------------------------------

Our third example, the PME, has a beautiful gradient flow structure in a different geometry: the \textbf{Wasserstein metric} from optimal transport theory.

\subsection{The Wasserstein Space}

Consider probability densities $\rho(x)$ with $\int\rho\,dx = 1$ and $\rho \geq 0$. The \textbf{Wasserstein distance} $W_2(\rho_1, \rho_2)$ measures the minimum ``cost'' to transport mass from distribution $\rho_1$ to $\rho_2$.

\marginnote{The Wasserstein metric comes from optimal transport. It measures the cost of moving mass from one distribution to another.}

Formally:
\begin{equation}
W_2(\rho_1, \rho_2)^2 = \inf_\gamma \int |x - y|^2 \, d\gamma(x, y)
\end{equation}
where the infimum is over ``transport plans'' $\gamma$ with marginals $\rho_1$ and $\rho_2$.

\subsection{PME as Gradient Flow}

Otto showed that the porous medium equation can be written as gradient flow in Wasserstein space:
\begin{equation}
\partial_t\rho = -\nabla_{W_2} E[\rho]
\end{equation}
where $E[\rho]$ is an entropy-like functional.

\marginnote{The PME minimizes an ``entropy'' in the Wasserstein metric. This is a geometric reformulation of nonlinear diffusion.}

For the PME with $m > 1$, the functional is:
\begin{equation}
E[\rho] = \frac{1}{m-1}\int \rho^m \, dx
\end{equation}

The gradient is taken with respect to the Wasserstein metric, not the $L^2$ metric.

\begin{workedbox}[Box 5.4: Gradient Flow Structure of the PME]
\textbf{The standard PME:}
\begin{equation}
\partial_t\rho = \nabla^2(\rho^m) = \nabla \cdot (\rho^{m-1}\nabla\rho)
\end{equation}

\textbf{The entropy functional:}
\begin{equation}
E[\rho] = \frac{1}{m-1}\int\rho^m\,dx
\end{equation}

\textbf{The Wasserstein gradient:}
In Wasserstein geometry, the gradient of $E$ at $\rho$ is:
\begin{equation}
(\nabla_{W_2}E)[\rho] = -\nabla\cdot\left(\rho\nabla\frac{\delta E}{\delta\rho}\right)
\end{equation}
where $\delta E/\delta\rho = \frac{m}{m-1}\rho^{m-1}$ is the functional derivative.

\textbf{Computation:}
\begin{equation}
(\nabla_{W_2}E)[\rho] = -\nabla\cdot\left(\rho\nabla\frac{m\rho^{m-1}}{m-1}\right) = -\nabla\cdot\left(\frac{m}{m-1}\rho^{m-1}\nabla\rho\right)
\end{equation}

\textbf{The PME:}
\begin{equation}
\partial_t\rho = -(\nabla_{W_2}E) = \nabla\cdot\left(\frac{m}{m-1}\rho^{m-1}\nabla\rho\right) \propto \nabla^2(\rho^m)
\end{equation}

\textbf{Conclusion:} The PME is gradient flow for the entropy $E$ in the Wasserstein metric.

\textbf{Physical meaning:} The density evolves to minimize the ``spread'' measured by $\int\rho^m$. For $m > 1$, this penalizes concentration and favors spreading, which is diffusion.
\end{workedbox}

\subsection{Implications}

The gradient flow structure implies entropy decreases along solutions:
\begin{equation}
\frac{dE}{dt} = -\int \rho \left|\nabla\frac{\delta E}{\delta\rho}\right|^2 dx \leq 0
\end{equation}

This is the analog of the c-theorem for the PME. The ``degrees of freedom'' (entropy) decrease as the density spreads.

%-------------------------------------------------------------------------------
\section{Looking Ahead}
\label{sec:ch5_preview}
%-------------------------------------------------------------------------------

This chapter established that parameter space has metric structure, and this metric extends to the full transseries-enlarged theory space.

The Fisher-Zamolodchikov metric measures distinguishability between theories. Gradient flow with respect to this metric underlies the c-theorem and related monotonicity results. The metric on the extended space has components measuring distinguishability along transseries directions, becoming large near Stokes lines.

\marginnote{Metrics tell us ``how far apart'' theories are. Connections tell us ``how to compare'' quantities at different theories.}

The next chapter introduces a richer geometric structure, namely \textbf{connections} on parameter space. Connections tell us how to ``parallel transport'' quantities from one theory to another. This leads to the key insight unifying the geometric and resurgent viewpoints: Stokes phenomena are monodromy.

%-------------------------------------------------------------------------------
\section*{Summary}
\addcontentsline{toc}{section}{Summary}
%-------------------------------------------------------------------------------

\begin{center}
\fbox{\parbox{0.85\textwidth}{
\textbf{The Fisher information metric:}
\begin{equation}
G_{ab}(g) = \int dx \, p(x;g)\,\frac{\partial\log p}{\partial g^a}\frac{\partial\log p}{\partial g^b}
\end{equation}
measures distinguishability between nearby theories.

\textbf{The Zamolodchikov metric} in QFT is the two-point function of operators that generate deformations.

\textbf{Extended theory space} includes transseries parameters $\sigma^n$. The metric extends to components $G_{\sigma^k\sigma^l}$ that become large near Stokes lines.

\textbf{Gradient flow:} When $\beta^a = -G^{ab}\partial_bV$, the RG flows ``downhill'' on the potential $V$.

\textbf{The c-theorem} (2D): The central charge $c$ decreases along RG trajectories. Analogous results hold in 3D (F-theorem) and 4D (a-theorem).

\textbf{The resurgent c-function} includes non-perturbative sectors. Monotonicity applies to the full transseries.

\textbf{The PME as gradient flow:} The porous medium equation minimizes an entropy functional in the Wasserstein metric, providing an alternative perspective on nonlinear diffusion.
}}
\end{center}
